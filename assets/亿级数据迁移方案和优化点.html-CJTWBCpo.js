import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,d as r,o as a}from"./app-DRDqcPuo.js";const n={};function i(c,t){return a(),o("div",null,t[0]||(t[0]=[r('<h1 id="亿级数据迁移方案和优化点" tabindex="-1"><a class="header-anchor" href="#亿级数据迁移方案和优化点"><span>亿级数据迁移方案和优化点</span></a></h1><p>异构库的不停机迁移方案</p><p>大型电商网站，用户对店铺的关注体系数据<br> 单项数据差不多有 8 亿多条</p><ul><li>编写双写代码<br> 凡是不停机迁移，一定要做双写<br> 为什么要双写？<br> 在迁移过程中，会有源源不断的新数据产生<br> 并且在整个读写逻辑迁移到新系统情况下，如果不双写的话，一旦某个用户已经迁移完毕了，后期这个用户又读写了，那么在新系统就会丢失数据</li></ul><p>在写新系统前，需要判断用户是否已经迁移完毕了<br> 如果迁移完毕了，再进行双写</p><ul><li>编写路由代码<br> 动态的去控制整个读写流程<br> 到底是读新系统还是老系统</li></ul><p>一般是配置在配置中心，阿波罗？Nacos？</p><ul><li>编写迁移代码<br> 整套系统，有</li></ul>',8)]))}const m=e(n,[["render",i],["__file","亿级数据迁移方案和优化点.html.vue"]]),s=JSON.parse(`{"path":"/inbox/%E4%BA%BF%E7%BA%A7%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88%E5%92%8C%E4%BC%98%E5%8C%96%E7%82%B9.html","title":"亿级数据迁移方案和优化点","lang":"zh-CN","frontmatter":{"title":"亿级数据迁移方案和优化点","tag":[],"description":"亿级数据迁移方案和优化点 异构库的不停机迁移方案 大型电商网站，用户对店铺的关注体系数据 单项数据差不多有 8 亿多条 编写双写代码 凡是不停机迁移，一定要做双写 为什么要双写？ 在迁移过程中，会有源源不断的新数据产生 并且在整个读写逻辑迁移到新系统情况下，如果不双写的话，一旦某个用户已经迁移完毕了，后期这个用户又读写了，那么在新系统就会丢失数据 在写...","head":[["meta",{"property":"og:url","content":"https://logycoconut.github.io/inbox/%E4%BA%BF%E7%BA%A7%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88%E5%92%8C%E4%BC%98%E5%8C%96%E7%82%B9.html"}],["meta",{"property":"og:site_name","content":"logycoconut's k-lab"}],["meta",{"property":"og:title","content":"亿级数据迁移方案和优化点"}],["meta",{"property":"og:description","content":"亿级数据迁移方案和优化点 异构库的不停机迁移方案 大型电商网站，用户对店铺的关注体系数据 单项数据差不多有 8 亿多条 编写双写代码 凡是不停机迁移，一定要做双写 为什么要双写？ 在迁移过程中，会有源源不断的新数据产生 并且在整个读写逻辑迁移到新系统情况下，如果不双写的话，一旦某个用户已经迁移完毕了，后期这个用户又读写了，那么在新系统就会丢失数据 在写..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-11T14:19:39.000Z"}],["meta",{"property":"article:modified_time","content":"2025-02-11T14:19:39.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"亿级数据迁移方案和优化点\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-11T14:19:39.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"logycoconut\\",\\"url\\":\\"https://logycoconut.github.io/\\"}]}"]]},"headers":[],"git":{"createdTime":1726242132000,"updatedTime":1739283579000,"contributors":[{"name":"logycoconut","username":"logycoconut","email":"logycoconut@foxmail.com","commits":2,"url":"https://github.com/logycoconut"}]},"readingTime":{"minutes":0.86,"words":258},"filePathRelative":"inbox/亿级数据迁移方案和优化点.md","localizedDate":"2024年9月13日","autoDesc":true}`);export{m as comp,s as data};
